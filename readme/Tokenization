Tokenization is a process used to break the data into small pieces.We going to break a paragraph into separate words!
